{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Solving Frozen Lake Problem Using Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, there is a frozen lake from your home to office, you should walk on the frozen lake\n",
    "to reach your office. But oops! there will be a hole in the frozen lake in between, so you have\n",
    "to be careful while walking in the frozen lake to avoid getting trapped at holes.\n",
    "Look at the below figure where, \n",
    "\n",
    "1. S is the starting position (Home)\n",
    "2. F is the Frozen lake where you can walk\n",
    "3. H is the Hole which you have to be so careful about\n",
    "4. G is the Goal (office)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us use our agent instead of you to find the correct way to reach the office.\n",
    "The agent goal is to find the optimal path to reach from S to G without getting trapped at H.\n",
    "How an agent can achieve this? We give +1 point as a reward to the agent if it correctly\n",
    "walks on the frozen lake and 0 points if it falls into the hole. So that agent could determine\n",
    "which is the right action. An agent will now try to find the optimal policy. Optimal policy\n",
    "implies taking the correct path which maximizes the agent reward. If the agent is\n",
    "maximizing the reward, apparently agent is learning to skip the hole and reach the\n",
    "destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, we import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the environment looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_function(policy, gamma=1.0):\n",
    "    \n",
    "    # initialize value table with zeros\n",
    "    value_table = np.zeros(env.nS)\n",
    "    \n",
    "    # set the threshold\n",
    "    threshold = 1e-10\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # copy the value table to the updated_value_table\n",
    "        updated_value_table = np.copy(value_table)\n",
    "\n",
    "        # for each state in the environment, select the action according to the policy and compute the value table\n",
    "        for state in range(env.nS):\n",
    "            action = policy[state]\n",
    "            \n",
    "            # build the value table with the selected action\n",
    "            value_table[state] = sum([trans_prob * (reward_prob + gamma * updated_value_table[next_state]) \n",
    "                        for trans_prob, next_state, reward_prob, _ in env.P[state][action]])\n",
    "            \n",
    "        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n",
    "            break\n",
    "    print(max(value_table))        \n",
    "    return value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we define a function called extract policy for extracting optimal policy from the optimal value function. \n",
    "i.e We calculate Q value using our optimal value function and pick up\n",
    "the actions which has the highest Q value for each state as the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_policy(value_table, gamma = 1.0):\n",
    " \n",
    "    # Initialize the policy with zeros\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        \n",
    "        # initialize the Q table for a state\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        \n",
    "        # compute Q value for all ations in the state\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_sr in env.P[state][action]: \n",
    "                trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "        \n",
    "        # Select the action which has maximum Q value as an optimal action of the state\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the function for performing policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,gamma = 1.0):\n",
    "    \n",
    "    # Initialize policy with zeros\n",
    "    old_policy = np.zeros(env.observation_space.n)   \n",
    "    no_of_iterations = 200000\n",
    "    \n",
    "    for i in range(no_of_iterations):\n",
    "        \n",
    "        # compute the value function\n",
    "        new_value_function = compute_value_function(old_policy, gamma)\n",
    "        \n",
    "        # Extract new policy from the computed value function\n",
    "        new_policy = extract_policy(new_value_function, gamma)\n",
    "   \n",
    "        # Then we check whether we have reached convergence i.e whether we found the optimal\n",
    "        # policy by comparing old_policy and new policy if it same we will break the iteration\n",
    "        # else we update old_policy with new_policy\n",
    "\n",
    "        if (np.all(old_policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        old_policy = new_policy\n",
    "    print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "    num_iteration = i+1\n",
    "    return new_policy, num_iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.0\n",
      "0.47619047619047616\n",
      "0.581864956617195\n",
      "0.6344650610150364\n",
      "0.638965077171239\n",
      "0.6390201480990125\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.91\n",
      "0.0\n",
      "0.47846889952153104\n",
      "0.589619225220666\n",
      "0.6469194521568005\n",
      "0.6526104878444627\n",
      "0.6526956051839923\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.92\n",
      "0.0\n",
      "0.4807692307692307\n",
      "0.597757570989898\n",
      "0.61161134904681\n",
      "0.6675373608765273\n",
      "0.6676692596813171\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.93\n",
      "0.0\n",
      "0.48309178743961345\n",
      "0.6063202589099419\n",
      "0.6221021237731418\n",
      "0.6840077016835137\n",
      "0.6842134598519889\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.94\n",
      "0.0\n",
      "0.4854368932038834\n",
      "0.6153555253116131\n",
      "0.633372916027441\n",
      "0.7023782746764737\n",
      "0.7027028151786884\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.95\n",
      "0.0\n",
      "0.4878048780487804\n",
      "0.6249221567295338\n",
      "0.6455443820891397\n",
      "0.7231533886158826\n",
      "0.723673636523662\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.96\n",
      "0.0\n",
      "0.4901960784313725\n",
      "0.6350932519887238\n",
      "0.6587686140543588\n",
      "0.74707941700924\n",
      "0.7479325578065195\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.97\n",
      "0.0\n",
      "0.4926108374384236\n",
      "0.6459618782539314\n",
      "0.6732415789578395\n",
      "0.7753262458737994\n",
      "0.7767703173543835\n",
      "Policy-Iteration converged at step 6.\n",
      "Policy-Iteration converged at step 6.\n",
      "0.98\n",
      "0.0\n",
      "0.495049504950495\n",
      "0.6576498713808822\n",
      "0.6892224963993108\n",
      "0.8098674522192051\n",
      "0.8124240674742482\n",
      "0.8127076667628887\n",
      "Policy-Iteration converged at step 7.\n",
      "Policy-Iteration converged at step 7.\n",
      "0.99\n",
      "0.0\n",
      "0.49751243781094523\n",
      "0.6703220814100899\n",
      "0.7070653425543743\n",
      "0.8543733518717862\n",
      "0.859209927424931\n",
      "0.8628374300861552\n",
      "Policy-Iteration converged at step 7.\n",
      "Policy-Iteration converged at step 7.\n",
      "1.0\n",
      "0.0\n",
      "0.49999999999999994\n",
      "0.6842105262702314\n",
      "0.7272727272306458\n",
      "0.9166666665686329\n",
      "0.9268292682003771\n",
      "0.9411764704995786\n",
      "Policy-Iteration converged at step 7.\n",
      "Policy-Iteration converged at step 7.\n"
     ]
    }
   ],
   "source": [
    "list_of_iterations = []\n",
    "for val in [round(i, 2) for i in np.arange(0.9, 1.01, 0.01)]:\n",
    "    print(val)\n",
    "    optimal_value_function, num_iter = policy_iteration(env=env,gamma=val)\n",
    "    list_of_iterations.append(num_iter)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
