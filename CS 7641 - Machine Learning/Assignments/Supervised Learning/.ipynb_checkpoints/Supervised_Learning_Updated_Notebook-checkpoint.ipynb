{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import parallel_backend\n",
    "from timeit import default_timer as timer\n",
    "from LoadData import LoadData\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import explained_variance_score, make_scorer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "TESTING = True\n",
    "DECISION_TREE = False\n",
    "SUPPORT_VECTOR = True\n",
    "NEURAL_NET = False\n",
    "K_NEAREST = False\n",
    "BOOSTING = False\n",
    "NORMALIZE_DATA = False\n",
    "USE_PCA = True\n",
    "DataSetName = \"Fashion-MNIST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    with plt.style.context('ggplot'):\n",
    "        if axes is None:\n",
    "            _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "        axes[0].set_title(title)\n",
    "        if ylim is not None:\n",
    "            axes[0].set_ylim(*ylim)\n",
    "        axes[0].set_xlabel(\"Training examples\")\n",
    "        axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "        train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "            learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                           train_sizes=train_sizes,\n",
    "                           return_times=True)\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        fit_times_mean = np.mean(fit_times, axis=1)\n",
    "        fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "        # Plot learning curve\n",
    "        axes[0].grid()\n",
    "        axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                             train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                             color=\"r\")\n",
    "        axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                             test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                             color=\"g\")\n",
    "        axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                     label=\"Training score\")\n",
    "        axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                     label=\"Cross-validation score\")\n",
    "        axes[0].legend(loc=\"best\")\n",
    "\n",
    "        # Plot n_samples vs fit_times\n",
    "        axes[1].grid()\n",
    "        axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "        axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                             fit_times_mean + fit_times_std, alpha=0.1)\n",
    "        axes[1].set_xlabel(\"Training examples\")\n",
    "        axes[1].set_ylabel(\"fit_times\")\n",
    "        axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "        # Plot fit_time vs score\n",
    "        axes[2].grid()\n",
    "        axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "        axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                             test_scores_mean + test_scores_std, alpha=0.1)\n",
    "        axes[2].set_xlabel(\"fit_times\")\n",
    "        axes[2].set_ylabel(\"Score\")\n",
    "        axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().absolute()\n",
    "if DataSetName == \"MNIST\":\n",
    "    training_data_path = \"{}/medium-mnist-train-data.csv\".format(cwd)\n",
    "    testing_data_path = \"{}/medium-mnist-test-data.csv\".format(cwd)\n",
    "else:\n",
    "    training_data_path = \"{}/medium-fashion-mnist-train-data.csv\".format(cwd)\n",
    "    testing_data_path = \"{}/medium-fashion-mnist-test-data.csv\".format(cwd)\n",
    "\n",
    "training_labels, training_data, _ = LoadData(training_data_path)\n",
    "testing_labels, testing_data, _ = LoadData(testing_data_path)\n",
    "\n",
    "Scaler = StandardScaler().fit(training_data)\n",
    "\n",
    "training_data = Scaler.transform(training_data)\n",
    "testing_data = Scaler.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "kernel = \"RBF\"\n",
    "\n",
    "title = r\"Learning Curves (SVM, Linear kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"SVM_{}_Learning_Curve.png\".format(kernel))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "kernel = \"Linear\"\n",
    "\n",
    "title = r\"Learning Curves (SVM, Linear kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"SVM_{}_Learning_Curve.png\".format(kernel))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "kernel = \"Poly\"\n",
    "\n",
    "title = r\"Learning Curves (SVM, Linear kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(kernel=kernel.lower(), verbose=1, gamma=0.001, max_iter=4000)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"SVM_{}_Learning_Curve.png\".format(kernel))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decision Trees\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "split = \"Entropy\"\n",
    "\n",
    "title = r\"Learning Curves (Decision Tree, {})\".format(split)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = DecisionTreeClassifier(criterion=split.lower(), max_depth=20)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "title = r\"Learning Curves (Decision Tree, {})\".format(split)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = DecisionTreeClassifier(criterion=split.lower(), max_depth=20)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"Decision_Tree_{}_Learning_Curve.png\".format(split))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "split = \"Gini\"\n",
    "\n",
    "title = r\"Learning Curves (Decision Tree, {})\".format(split)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = DecisionTreeClassifier(criterion=split.lower(), max_depth=20)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "title = r\"Learning Curves (Decision Tree, {})\".format(split)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = DecisionTreeClassifier(criterion=split.lower(), max_depth=20)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"Decision_Tree_{}_Learning_Curve.png\".format(split))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boosted Decision Trees\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "title = r\"Learning Curves (Gradient Boosted Decision Tree)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier(max_depth=3, verbose=2)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (Gradient Boosted Decision Tree)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = GradientBoostingClassifier(max_depth=3, verbose=2)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"Boosted_Decision_Tree_Learning_Curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Neural Network\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "solver='Adam'\n",
    "\n",
    "title = r\"Learning Curves (Neural Network, Adam kernel)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MLPClassifier(solver=solver.lower(), max_iter=200, verbose=1, hidden_layer_sizes=(100,))\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (Neural Network, Adam kernel)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MLPClassifier(solver='adam', max_iter=200, verbose=1, hidden_layer_sizes=(100,))\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"Neural_Network_{}_Learning_Curve.png\".format(solver))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "solver='SGD'\n",
    "\n",
    "title = r\"Learning Curves (Neural Network, Adam kernel)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MLPClassifier(solver=solver.lower(), max_iter=200, verbose=1, hidden_layer_sizes=(100,))\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (Neural Network, Adam kernel)\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = MLPClassifier(solver='adam', max_iter=200, verbose=1, hidden_layer_sizes=(100,))\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"Neural_Network_{}_Learning_Curve.png\".format(solver))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KNN\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "k_val = 3\n",
    "\n",
    "title = r\"Learning Curves (KNN, {})\".format(k_val)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = KNeighborsClassifier(n_neighbors=k_val)\n",
    "plot_learning_curve(estimator, title, training_data, training_labels, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "\n",
    "title = r\"Learning Curves (KNN,{})\".format(k_val)\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = KNeighborsClassifier(n_neighbors=k_val)\n",
    "plot_learning_curve(estimator, title, training_data[:3000], training_labels[:3000], axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=4)\n",
    "plt.savefig(\"SVM_{}_Learning_Curve.png\".format(k_val))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().absolute()\n",
    "if DataSetName == \"MNIST\":\n",
    "    training_data_path = \"{}/mnist-train-data.csv\".format(cwd)\n",
    "    testing_data_path = \"{}/mnist-test-data.csv\".format(cwd)\n",
    "else:\n",
    "    training_data_path = \"{}/fashion-mnist-train-data.csv\".format(cwd)\n",
    "    testing_data_path = \"{}/fashion-mnist-test-data.csv\".format(cwd)\n",
    "\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    training_labels, training_data, _ = LoadData(training_data_path, normalize=NORMALIZE_DATA)\n",
    "    testing_labels, testing_data, _ = LoadData(testing_data_path, normalize=NORMALIZE_DATA)\n",
    "\n",
    "Scaler = StandardScaler().fit(training_data)\n",
    "        \n",
    "training_data = Scaler.transform(training_data)\n",
    "testing_data = Scaler.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAINING TIME\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = []\n",
    "runtime = [0.0]\n",
    "accuracy = [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(max_depth=3)\n",
    "clf.fit(training_data[:int((60000 * 0.1)), :], training_labels[:int((60000 * 0.1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.validation_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with parallel_backend('threading'):\n",
    "    clf = HistGradientBoostingClassifier(max_depth=3)\n",
    "#     clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, verbose=3, learning_rate=0.1)\n",
    "    for i in range(1, 11, 1):\n",
    "        print(\"{} - Training Size: {}%\".format(\"Boosted\", (i * 10)))\n",
    "        start_time = timer()\n",
    "        with parallel_backend('threading'):\n",
    "            clf.fit(training_data[:int((60000 * (0.1 * i))), :], training_labels[:int((60000 * (0.1 * i)))])\n",
    "        end_time = timer()\n",
    "        elapsed_time = end_time - start_time\n",
    "        if i == 10:\n",
    "                classifier_list.append(clf)\n",
    "        print(elapsed_time)\n",
    "        accuracy.append(clf.score(testing_data, testing_labels))\n",
    "        runtime.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.asarray(accuracy)\n",
    "runtime = np.asarray(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.tofile('boosted_accuracy_{}.csv'.format(DataSetName),sep=',',format='%.3f')\n",
    "runtime.tofile('boosted_runtime_{}.csv'.format(DataSetName),sep=',',format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(classifier_list)):\n",
    "    disp = plot_confusion_matrix(classifier_list[i], testing_data, testing_labels, values_format=\".4g\")\n",
    "    disp.figure_.suptitle(\"{} Confusion Matrix\".format(\"Boosted\"))\n",
    "    print(\"TESTING\")\n",
    "    print(disp.confusion_matrix)\n",
    "    plt.savefig(\"{}_ConfusionMatrix_{}.png\".format(\"Boosted\", DataSetName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"tab:orange\", \"tab:blue\", \"tab:green\", \"tab:red\"]\n",
    "\n",
    "run = runtime\n",
    "acc = accuracy\n",
    "\n",
    "\n",
    "with plt.style.context('ggplot'):\n",
    "    fig0, ax0 = plt.subplots()\n",
    "    ax0.set_xlabel(\"Percent of Training Set\")\n",
    "    ax0.set_ylabel(\"Accuracy (%)\", color='tab:orange')\n",
    "    ax0.set_title(\"Accuracy vs Training Set Size vs Training Time {} \\n {}\".format(\"Boosted-Hist\", DataSetName))\n",
    "    ax0.tick_params(axis='y', labelcolor=\"black\")\n",
    "    ax0.set_ylim(0, 1.1)\n",
    "    ax3 = ax0.twinx()\n",
    "    ax3.set_ylabel(\"Training Time (s)\", color=\"tab:blue\")\n",
    "    ax3.set_ylim(0, max(max(runtime), max(runtime)) + 10)\n",
    "    ax3.tick_params(axis='y', labelcolor=\"black\")\n",
    "    for i in range(1):        \n",
    "        ax0.plot([i for i in range(11)], acc, colors[i], marker='o', label=\"Boosted-Hist Accuracy\")\n",
    "        ax3.plot([i for i in range(11)], run, colors[i+1], marker=\"1\", label=\"{} training-time\".format(\"Boosted-Hist Training Time\"))\n",
    "    fig0.tight_layout()\n",
    "    directory = \"{}/Training_{}_{}_Hist_Set_Size_Impact_vs_Training_Time.png\".format(cwd, \"Boosted\", DataSetName)\n",
    "    plt.savefig(directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
